{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"fomm-unet-sample-pro.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f0284bf8b49243e89560e86742f84f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2eeb4771701e4c1aad5a0be30ee6d909","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1069228d94f64c5180c730204765888e","IPY_MODEL_ca286c50dad34b14b7c1ce9460a7baba","IPY_MODEL_9fa82918ec4643cf988211e559177c89"]}},"2eeb4771701e4c1aad5a0be30ee6d909":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1069228d94f64c5180c730204765888e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b928e3c411294fc6a901bf22a88c361e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fe0bbb1e2d44468d8d885fd97cbabcda"}},"ca286c50dad34b14b7c1ce9460a7baba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e005599dc4174d68892319a197e66caa","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_424fa7d4a4174ed382932607f65c0e32"}},"9fa82918ec4643cf988211e559177c89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_69a821b2b83042b48d1398f6fe49085d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:07&lt;00:00, 111MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3658836cff01443db5a840272e872892"}},"b928e3c411294fc6a901bf22a88c361e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"fe0bbb1e2d44468d8d885fd97cbabcda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e005599dc4174d68892319a197e66caa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"424fa7d4a4174ed382932607f65c0e32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69a821b2b83042b48d1398f6fe49085d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3658836cff01443db5a840272e872892":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"uRKy3UlYbucV"},"source":["# Initialize training project"]},{"cell_type":"markdown","metadata":{"id":"PDeHeICibw1h"},"source":["## Checking gpu in colab pro"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"3NaBJpMZxbr3","executionInfo":{"status":"ok","timestamp":1633098013001,"user_tz":300,"elapsed":2778,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"715c4852-e4cb-4cf3-aa65-b5cf10f8320e"},"source":["import torch\n","\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"rZot2psgbzs5"},"source":["## Entering the google drive data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEMAIh7Ifa9s","executionInfo":{"status":"ok","timestamp":1633098034414,"user_tz":300,"elapsed":21416,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"ad90aa77-712f-4c34-9503-d1dcb3d6f71c"},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"pk4-SHPOWEIu"},"source":["## Seeds para reproducibilidad\n"]},{"cell_type":"code","metadata":{"id":"q5ven54VJgzg"},"source":["%matplotlib inline\n","import numpy as np \n","import torch\n","import random\n","import os\n","from skimage import io, img_as_float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06tqtCEHWZ68"},"source":["def set_seed(seed):\n","  random.seed(seed)        \n","  torch.manual_seed(seed)  \n","\n","set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l83SMtXfb32m"},"source":["## Partitioning on mini batches\n","<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/7C3SV7Q/sampling.jpg\" alt=\"sampling\" border=\"0\"></a>"]},{"cell_type":"markdown","metadata":{"id":"-ZcYgo32b8la"},"source":["Remember that this script was developed in a google collab context. You can change the path to the files in the ```data/vox-png``` folder.\n","\n","The ```batches.txt``` file is in charge of saving in numbers the indexes of files that will be used in each training group"]},{"cell_type":"code","metadata":{"id":"NN1x27yWmoa-"},"source":["main_path = '/content/drive/MyDrive/Tesis/first-order-model-6c/data/vox-png' #Cambiar la ruta de ser necesario\n","batch_index_path = '/content/drive/MyDrive/first-order-model-6c'\n","fbatches='batches.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4yR-xedMhM1"},"source":["def directory_iter(files, batch_size, shuffle=True):\n","  n = files.shape[0]\n","  \n","  if shuffle:\n","    indices = np.random.permutation(n)\n","  else:\n","    indices = range(n)\n","\n","  for i in range(0, n, batch_size):\n","    batch_indices = indices[i:i+batch_size if i+batch_size <=n else n]\n","    #files_batch = files[batch_indices]\n","    yield batch_indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwpOIVifMmBr"},"source":["We will save the indexes in the file ```batches.txt```"]},{"cell_type":"code","metadata":{"id":"USiYGTuVMmLh"},"source":["files=np.array(os.listdir(main_path))\n","batch_size = 400\n","total_samples = 0\n","mini_directory_bt=[]\n","\n","if os.path.isfile(os.path.join(batch_index_path,fbatches)):\n","  os.remove(os.path.join(batch_index_path,fbatches))\n","\n","with open(os.path.join(batch_index_path,fbatches), 'w') as f:\n","  for i, batch_indices in enumerate(directory_iter(files, batch_size), 1):\n","    total_samples += batch_indices.shape[0]\n","    #print(f'Batch {i} has size {batch_indices.shape[0]}') #To show the size of each lot\n","    for index in batch_indices:\n","      f.write(\"%s \" % index)\n","    f.write(\"\\n\")\n","\n","if total_samples == files.shape[0]:\n","  print(':) The total number of samples per batch is correct.')\n","else:\n","  print(':( The total number of samples per batch differs from the total number of samples.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywys3N7GTSeV"},"source":["# TRAINING"]},{"cell_type":"markdown","metadata":{"id":"PRoiHelNcOSJ"},"source":["Remember that this script was developed in a google collab context. You can change the path ```cd``` but in the same folder of the present model  **first-order-model-unet**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tqpBN-ZpFt-","executionInfo":{"status":"ok","timestamp":1633098036554,"user_tz":300,"elapsed":983,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"25790c01-da38-4a6d-a2a3-adaec582a6d7"},"source":["cd drive/MyDrive/Tesis/first-order-model-unet/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/18KpKDna72zchP8wggK4JGnxV7TQbcfCC/Tesis/first-order-model-unet\n"]}]},{"cell_type":"markdown","metadata":{"id":"_52RrqPHcUGi"},"source":["Installing First Order Model repository requirements"]},{"cell_type":"code","metadata":{"id":"9i7V5Pq5cVzh"},"source":["#!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-lfu-rg1oxJ"},"source":["from shutil import copy\n","import imageio\n","import numpy as np\n","import sys\n","import uuid\n","import yaml\n","import torch\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import warnings\n","from time import gmtime, strftime\n","from skimage import img_as_ubyte\n","from ctypes import cdll\n","from train import train\n","from modules.generator import OcclusionAwareGenerator\n","from modules.discriminator import MultiScaleDiscriminator\n","from modules.keypoint_detector import KPDetector\n","from frames_dataset import FramesDataset,FramesDatasetPartitioning\n","from modules.util import DownBlock2d\n","from tqdm import trange\n","from torch.utils.data import DataLoader\n","from logger import Logger\n","from modules.model import GeneratorFullModel, DiscriminatorFullModel\n","from torch.optim.lr_scheduler import MultiStepLR\n","from sync_batchnorm import DataParallelWithCallback\n","from frames_dataset import DatasetRepeater\n","#from demo import load_checkpoints, make_animation, load_checkpoints_Unet_3\n","from demo import load_checkpoints, make_animation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZzgjZOMcYaL"},"source":["Reading the ```batches.txt``` file and the **number of batch**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Kk3kfiA14rN","executionInfo":{"status":"ok","timestamp":1633098133305,"user_tz":300,"elapsed":85394,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"134060ce-2cc7-4d0f-bb4d-e27ab9a35f99"},"source":["num_batch=8\n","\n","batchf = './batches.txt'\n","config = './config/vox-adv-256.yaml'\n","\n","\n","batch_list=[]\n","with open(batchf, 'r') as f:\n","  for line in f.readlines():\n","    batch_list.append(list(map(int, line.rstrip().split(\" \"))))\n","\n","\n","with open(config) as f:\n","        config = yaml.load(f)\n","\n","dataset = FramesDatasetPartitioning(is_train=1, **config['dataset_params'],batches_list=batch_list[num_batch])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use predefined train-test split.\n"]}]},{"cell_type":"markdown","metadata":{"id":"Tr5CPeH4cZ4Z"},"source":["Debug the index file and the name of each video"]},{"cell_type":"code","metadata":{"id":"ReLLQGgM49aL"},"source":["print(batch_list[num_batch])\n","print(dataset.videos)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c5ZW9g7eccb8"},"source":["Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["f0284bf8b49243e89560e86742f84f7b","2eeb4771701e4c1aad5a0be30ee6d909","1069228d94f64c5180c730204765888e","ca286c50dad34b14b7c1ce9460a7baba","9fa82918ec4643cf988211e559177c89","b928e3c411294fc6a901bf22a88c361e","fe0bbb1e2d44468d8d885fd97cbabcda","e005599dc4174d68892319a197e66caa","424fa7d4a4174ed382932607f65c0e32","69a821b2b83042b48d1398f6fe49085d","3658836cff01443db5a840272e872892"]},"id":"Mfo7ApPT495K","executionInfo":{"status":"ok","timestamp":1633113447361,"user_tz":300,"elapsed":15314063,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"78f4b847-9623-403e-98be-de1b289f69c9"},"source":["warnings.filterwarnings(\"ignore\")\n","\n","config = './config/vox-adv-256.yaml'\n","device_ids = [0]\n","#checkpoint = './models/vox-adv-cpk.pth.tar'\n","checkpoint = None\n","log_dir = './logs'\n","batchf= './batches.txt'\n","\n","\n","batch_list=[]\n","with open(batchf, 'r') as f:\n","  for line in f.readlines():\n","    batch_list.append(list(map(int, line.rstrip().split(\" \"))))\n","\n","if __name__ == \"__main__\":\n","    \n","    with open(config) as f:\n","        config = yaml.load(f)\n","        \n","    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        generator.to(device_ids[0])\n","\n","    discriminator = MultiScaleDiscriminator(**config['model_params']['discriminator_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        discriminator.to(device_ids[0])\n","\n","    kp_detector = KPDetector(**config['model_params']['kp_detector_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        kp_detector.to(device_ids[0])\n","            \n","    #dataset = FramesDatasetPartitioning(is_train=1, **config['dataset_params'],batches_list=batch_list[num_batch])\n","\n","    print(\"Training...\")\n","\n","    train_params = config['train_params']\n","\n","    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=train_params['lr_generator'], betas=(0.5, 0.999))\n","    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=train_params['lr_discriminator'], betas=(0.5, 0.999))\n","    optimizer_kp_detector = torch.optim.Adam(kp_detector.parameters(), lr=train_params['lr_kp_detector'], betas=(0.5, 0.999))\n","\n","    if checkpoint is not None:\n","        start_epoch = load_cpk(checkpoint, generator, discriminator, kp_detector, optimizer_generator, optimizer_discriminator, None if train_params['lr_kp_detector'] == 0 else optimizer_kp_detector)\n","    else:\n","        start_epoch = 0\n","\n","    scheduler_generator = MultiStepLR(optimizer_generator, train_params['epoch_milestones'], gamma=0.1, last_epoch=start_epoch - 1)\n","\n","    scheduler_discriminator = MultiStepLR(optimizer_discriminator, train_params['epoch_milestones'], gamma=0.1, last_epoch=start_epoch - 1)\n","\n","    scheduler_kp_detector = MultiStepLR(optimizer_kp_detector, train_params['epoch_milestones'], gamma=0.1, last_epoch=-1 + start_epoch * (train_params['lr_kp_detector'] != 0))\n","\n","    if 'num_repeats' in train_params or train_params['num_repeats'] != 1:\n","        dataset = DatasetRepeater(dataset, train_params['num_repeats'])\n","\n","    dataloader = DataLoader(dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=6, drop_last=True)\n","\n","    generator_full = GeneratorFullModel(kp_detector, generator, discriminator, train_params)\n","    discriminator_full = DiscriminatorFullModel(kp_detector, generator, discriminator, train_params)\n","\n","    if torch.cuda.is_available():\n","        generator_full = DataParallelWithCallback(generator_full, device_ids=device_ids)\n","        discriminator_full = DataParallelWithCallback(discriminator_full, device_ids=device_ids)\n","\n","    with Logger(log_dir=log_dir, visualizer_params=config['visualizer_params'], checkpoint_freq=train_params['checkpoint_freq']) as logger:\n","        for epoch in trange(start_epoch, train_params['num_epochs']):\n","            for x in dataloader:\n","                #print(dataloader)\n","                losses_generator, generated = generator_full(x)\n","\n","                loss_values = [val.mean() for val in losses_generator.values()]\n","                loss = sum(loss_values)\n","\n","                loss.backward()\n","                optimizer_generator.step()\n","                optimizer_generator.zero_grad()\n","                optimizer_kp_detector.step()\n","                optimizer_kp_detector.zero_grad()\n","\n","                if train_params['loss_weights']['generator_gan'] != 0:\n","                    optimizer_discriminator.zero_grad()\n","                    losses_discriminator = discriminator_full(x, generated)\n","                    loss_values = [val.mean() for val in losses_discriminator.values()]\n","                    loss = sum(loss_values)\n","\n","                    loss.backward()\n","                    optimizer_discriminator.step()\n","                    optimizer_discriminator.zero_grad()\n","                else:\n","                    losses_discriminator = {}\n","\n","                losses_generator.update(losses_discriminator)\n","                losses = {key: value.mean().detach().data.cpu().numpy() for key, value in losses_generator.items()}\n","                logger.log_iter(losses=losses)\n","\n","            scheduler_generator.step()\n","            scheduler_discriminator.step()\n","            scheduler_kp_detector.step()\n","            \n","            logger.log_epoch(epoch, {'generator': generator, 'discriminator': discriminator, 'kp_detector': kp_detector, 'optimizer_generator': optimizer_generator, 'optimizer_discriminator': optimizer_discriminator, 'optimizer_kp_detector': optimizer_kp_detector}, inp=x, out=generated)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0284bf8b49243e89560e86742f84f7b","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [4:14:56<00:00, 1529.67s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"OUmH3gW9TB9v"},"source":["# CHECKPOINT READING"]},{"cell_type":"markdown","metadata":{"id":"b7vBsydwc0t9"},"source":["Showing the current training model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZRbPvAF4ip9","executionInfo":{"status":"ok","timestamp":1633113696088,"user_tz":300,"elapsed":2361,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"575498e8-83b1-40e9-dc7f-f34344d77a7b"},"source":["checkpoint_path = './logs/00000009-checkpoint.pth.tar'\n","\n","config_path = './config/vox-adv-256.yaml'\n","generator1, kp_detector1 = load_checkpoints(config_path,checkpoint_path)\n","print(kp_detector1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataParallelWithCallback(\n","  (module): KPDetector(\n","    (predictor): HourglassMod(\n","      (encoder): EncoderMod(\n","        (down_blocks): ModuleList(\n","          (0): DownBlock2dMod(\n","            (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (1): DownBlock2dMod(\n","            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (2): DownBlock2dMod(\n","            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (3): DownBlock2dMod(\n","            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (4): DownBlock2dMod(\n","            (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","        )\n","      )\n","      (decoder): DecoderMod(\n","        (up_blocks): ModuleList(\n","          (0): UpBlock2dMod(\n","            (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (1): UpBlock2dMod(\n","            (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (2): UpBlock2dMod(\n","            (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (3): UpBlock2dMod(\n","            (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (4): UpBlock2dMod(\n","            (conv1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","      )\n","    )\n","    (kp): Conv2d(35, 10, kernel_size=(7, 7), stride=(1, 1))\n","    (jacobian): Conv2d(35, 40, kernel_size=(7, 7), stride=(1, 1))\n","    (down): AntiAliasInterpolation2d()\n","  )\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"6A2JrGCbTJfn"},"source":["# ANIMATION"]},{"cell_type":"markdown","metadata":{"id":"1B4yQVqvc6vD"},"source":["If we want a test of the model with the generated checkpoint, a source image and a driver video are saved"]},{"cell_type":"code","metadata":{"id":"6X4iH5BZTF3N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633113704154,"user_tz":300,"elapsed":3018,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"50b1c36d-d679-4591-f6f5-faca0d2ec402"},"source":["from animate import normalize_kp\n","from skimage.transform import resize\n","from moviepy import editor\n","from tqdm import tqdm\n","import os"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2654208/45929032 bytes (5.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b6553600/45929032 bytes (14.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10493952/45929032 bytes (22.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b14491648/45929032 bytes (31.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18628608/45929032 bytes (40.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22552576/45929032 bytes (49.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b26648576/45929032 bytes (58.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b30728192/45929032 bytes (66.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b34889728/45929032 bytes (76.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39026688/45929032 bytes (85.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b43155456/45929032 bytes (94.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkR-WCpTTK41","executionInfo":{"status":"ok","timestamp":1633113803473,"user_tz":300,"elapsed":10965,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"c7cc1a3d-7589-4bc7-fda8-fb4675bc0c3d"},"source":["adapt_movement_scale = True \n","cpu = False\n","relative = True\n","\n","D_VIDEO_PATH = 'logs/animation/video_d.mp4'\n","S_IMAGE_PATH = 'logs/animation/image_s.png'\n","\n","G_VIDEO_PATH = 'logs/animation/video_g.mp4'\n","if os.path.isfile(G_VIDEO_PATH):\n","  os.remove(G_VIDEO_PATH)\n","\n","source_image = imageio.imread(S_IMAGE_PATH)\n","driving_video = imageio.mimread(D_VIDEO_PATH, memtest=False)\n","\n","reader = imageio.get_reader(D_VIDEO_PATH)\n","driving_video_FPS = reader.get_meta_data()['fps']\n","\n","source_image = resize(source_image, (256, 256))[..., :3]\n","driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n","driving_video_audio = editor.AudioFileClip(D_VIDEO_PATH)\n","\n","\n","with torch.no_grad():\n","  predictions = []\n","  source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n","  if not cpu:\n","    source = source.cuda()\n","    driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n","    kp_source = kp_detector1(source)\n","    kp_driving_initial = kp_detector1(driving[:, :, 0])\n","\n","    for frame_idx in tqdm(range(driving.shape[2])):\n","      percentage = (frame_idx + 1)/ driving.shape[2]\n","      #yield \"data:\" + str(percentage) + \"\\n\\n\"\n","      driving_frame = driving[:, :, frame_idx]\n","      if not cpu:\n","        driving_frame = driving_frame.cuda()\n","      kp_driving = kp_detector1(driving_frame)\n","      kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n","              kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n","              use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n","      out = generator1(source, kp_source=kp_source, kp_driving=kp_norm)\n","\n","      predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 150/150 [00:06<00:00, 23.26it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"zcf_VqS7c9A2"},"source":["Saving de video generated"]},{"cell_type":"code","metadata":{"id":"6TWciTVJTNHH"},"source":["imageio.mimsave(G_VIDEO_PATH, [img_as_ubyte(frame) for frame in predictions], fps=driving_video_FPS)"],"execution_count":null,"outputs":[]}]}