{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"fomm-sample-pro.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1fa9c3c41a1044acaeac4faa327f9042":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fd2156b3e68d47c191a27ad4b2a33114","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_47f40085f5154d8ba329f0708eb884df","IPY_MODEL_cceb25224da44b4ea4114e86245fcb22","IPY_MODEL_9d649f49cc8640abaa114ed0e509dd10"]}},"fd2156b3e68d47c191a27ad4b2a33114":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"47f40085f5154d8ba329f0708eb884df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dd7bb2f04c4a489988a886a1fce2fc13","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87109ea8797a4b809522201e696f0ce4"}},"cceb25224da44b4ea4114e86245fcb22":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_40b7f93272d94c86a36b14e19015304a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6ef95e7945be4f56b954df56f92fc3bb"}},"9d649f49cc8640abaa114ed0e509dd10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6995a1a5425241d4b80f054d94a3b44b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [00:03&lt;00:00, 156MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0532d63781e643e29f202ee7f7d18b78"}},"dd7bb2f04c4a489988a886a1fce2fc13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"87109ea8797a4b809522201e696f0ce4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40b7f93272d94c86a36b14e19015304a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6ef95e7945be4f56b954df56f92fc3bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6995a1a5425241d4b80f054d94a3b44b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0532d63781e643e29f202ee7f7d18b78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"A16aCivSvG24"},"source":["# Initialize training project"]},{"cell_type":"markdown","metadata":{"id":"eUFrEhYyvJL0"},"source":["## Checking gpu in colab pro"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"3NaBJpMZxbr3","executionInfo":{"status":"ok","timestamp":1633096609966,"user_tz":300,"elapsed":2201,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"71799a5e-2788-43b8-b711-cc9788f507a9"},"source":["import torch\n","\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla P100-PCIE-16GB'"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"1beVMihzvLXo"},"source":["## Entering the google drive data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEMAIh7Ifa9s","executionInfo":{"status":"ok","timestamp":1633096642096,"user_tz":300,"elapsed":19414,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"bb9b87a9-8c81-4483-fc31-d0f4a03732e5"},"source":["from google.colab import drive\n","drive.mount('/content/drive',force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"pk4-SHPOWEIu"},"source":["## Seeds para reproducibilidad\n"]},{"cell_type":"code","metadata":{"id":"q5ven54VJgzg"},"source":["%matplotlib inline\n","import numpy as np \n","import torch\n","import random\n","import os\n","from skimage import io, img_as_float32"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06tqtCEHWZ68"},"source":["def set_seed(seed):\n","  random.seed(seed)        \n","  torch.manual_seed(seed)  \n","\n","set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dGHvHB73vkhI"},"source":["## Partitioning on mini batches\n","<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/7C3SV7Q/sampling.jpg\" alt=\"sampling\" border=\"0\"></a>\n"]},{"cell_type":"markdown","metadata":{"id":"HQ5sbQlTvoMG"},"source":["Remember that this script was developed in a google collab context. You can change the path to the files in the ```data/vox-png``` folder.\n","\n","The ```batches.txt``` file is in charge of saving in numbers the indexes of files that will be used in each training group"]},{"cell_type":"code","metadata":{"id":"NN1x27yWmoa-"},"source":["main_path = '/content/drive/MyDrive/Tesis/first-order-model-6c/data/vox-png'\n","batch_index_path = '/content/drive/MyDrive/first-order-model-6c'\n","fbatches='batches.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X4yR-xedMhM1"},"source":["def directory_iter(files, batch_size, shuffle=True):\n","  n = files.shape[0]\n","  \n","  if shuffle:\n","    indices = np.random.permutation(n)\n","  else:\n","    indices = range(n)\n","\n","  for i in range(0, n, batch_size):\n","    batch_indices = indices[i:i+batch_size if i+batch_size <=n else n]\n","    #files_batch = files[batch_indices]\n","    yield batch_indices"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwpOIVifMmBr"},"source":["We will save the indexes in the file ```batches.txt```"]},{"cell_type":"code","metadata":{"id":"USiYGTuVMmLh"},"source":["files=np.array(os.listdir(main_path))\n","batch_size = 400\n","total_samples = 0\n","mini_directory_bt=[]\n","\n","if os.path.isfile(os.path.join(batch_index_path,fbatches)):\n","  os.remove(os.path.join(batch_index_path,fbatches))\n","\n","with open(os.path.join(batch_index_path,fbatches), 'w') as f:\n","  for i, batch_indices in enumerate(directory_iter(files, batch_size), 1):\n","    total_samples += batch_indices.shape[0]\n","    #print(f'Batch {i} has size {batch_indices.shape[0]}') #To show the size of each lot\n","    for index in batch_indices:\n","      f.write(\"%s \" % index)\n","    f.write(\"\\n\")\n","\n","if total_samples == files.shape[0]:\n","  print(':) The total number of samples per batch is correct.')\n","else:\n","  print(':( The total number of samples per batch differs from the total number of samples.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywys3N7GTSeV"},"source":["# TRAINING"]},{"cell_type":"markdown","metadata":{"id":"psRM1rSHwHyV"},"source":["Remember that this script was developed in a google collab context. You can change the path ```cd``` but in the same folder of the present model  **first-order-model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tqpBN-ZpFt-","executionInfo":{"status":"ok","timestamp":1633096648560,"user_tz":300,"elapsed":725,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"c170ea29-05cb-4e5e-8c57-2875c205413f"},"source":["cd drive/MyDrive/Tesis/first-order-model/"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/18KpKDna72zchP8wggK4JGnxV7TQbcfCC/Tesis/first-order-model\n"]}]},{"cell_type":"markdown","metadata":{"id":"_EM7sD8pwLtT"},"source":["Installing First Order Model repository requirements"]},{"cell_type":"code","metadata":{"id":"RCDAp2ncwL5n"},"source":["#!pip install -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-lfu-rg1oxJ"},"source":["from shutil import copy\n","import imageio\n","import numpy as np\n","import sys\n","import uuid\n","import yaml\n","import torch\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","import warnings\n","from time import gmtime, strftime\n","from skimage import img_as_ubyte\n","from ctypes import cdll\n","from train import train\n","from modules.generator import OcclusionAwareGenerator\n","from modules.discriminator import MultiScaleDiscriminator\n","from modules.keypoint_detector import KPDetector\n","from frames_dataset import FramesDataset,FramesDatasetPartitioning\n","from modules.util import DownBlock2d\n","from tqdm import trange\n","from torch.utils.data import DataLoader\n","from logger import Logger\n","from modules.model import GeneratorFullModel, DiscriminatorFullModel\n","from torch.optim.lr_scheduler import MultiStepLR\n","from sync_batchnorm import DataParallelWithCallback\n","from frames_dataset import DatasetRepeater\n","#from demo import load_checkpoints, make_animation, load_checkpoints_Unet_3\n","from demo import load_checkpoints, make_animation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gTMowlPrwSQn"},"source":["Reading the ```batches.txt``` file and the **number of batch**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Kk3kfiA14rN","executionInfo":{"status":"ok","timestamp":1633047718169,"user_tz":300,"elapsed":406,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"c2028306-a11d-475e-a9a5-aed6642b79a1"},"source":["num_batch=8\n","\n","batchf = './batches.txt'\n","config = './config/vox-adv-256.yaml'\n","\n","\n","batch_list=[]\n","with open(batchf, 'r') as f:\n","  for line in f.readlines():\n","    batch_list.append(list(map(int, line.rstrip().split(\" \"))))\n","\n","\n","with open(config) as f:\n","        config = yaml.load(f)\n","\n","dataset = FramesDatasetPartitioning(is_train=1, **config['dataset_params'],batches_list=batch_list[num_batch])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Use predefined train-test split.\n"]}]},{"cell_type":"markdown","metadata":{"id":"qM-R3ZfcbSAC"},"source":["Debug the index file and the name of each video"]},{"cell_type":"code","metadata":{"id":"ReLLQGgM49aL"},"source":["print(batch_list[num_batch])\n","print(dataset.videos)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OAr5pgZybV7x"},"source":["Training"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["1fa9c3c41a1044acaeac4faa327f9042","fd2156b3e68d47c191a27ad4b2a33114","47f40085f5154d8ba329f0708eb884df","cceb25224da44b4ea4114e86245fcb22","9d649f49cc8640abaa114ed0e509dd10","dd7bb2f04c4a489988a886a1fce2fc13","87109ea8797a4b809522201e696f0ce4","40b7f93272d94c86a36b14e19015304a","6ef95e7945be4f56b954df56f92fc3bb","6995a1a5425241d4b80f054d94a3b44b","0532d63781e643e29f202ee7f7d18b78"]},"id":"Mfo7ApPT495K","executionInfo":{"status":"ok","timestamp":1633064594992,"user_tz":300,"elapsed":16869987,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"f807a6db-2b87-49df-fa2e-ecb140331684"},"source":["warnings.filterwarnings(\"ignore\")\n","\n","config = './config/vox-adv-256.yaml'\n","device_ids = [0]\n","#checkpoint = './models/vox-adv-cpk.pth.tar'\n","checkpoint = None\n","log_dir = './logs'\n","batchf= './batches.txt'\n","\n","\n","batch_list=[]\n","with open(batchf, 'r') as f:\n","  for line in f.readlines():\n","    batch_list.append(list(map(int, line.rstrip().split(\" \"))))\n","\n","if __name__ == \"__main__\":\n","    \n","    with open(config) as f:\n","        config = yaml.load(f)\n","        \n","    generator = OcclusionAwareGenerator(**config['model_params']['generator_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        generator.to(device_ids[0])\n","\n","    discriminator = MultiScaleDiscriminator(**config['model_params']['discriminator_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        discriminator.to(device_ids[0])\n","\n","    kp_detector = KPDetector(**config['model_params']['kp_detector_params'], **config['model_params']['common_params'])\n","\n","    if torch.cuda.is_available():\n","        kp_detector.to(device_ids[0])\n","            \n","    #dataset = FramesDatasetPartitioning(is_train=1, **config['dataset_params'],batches_list=batch_list[num_batch])\n","\n","    print(\"Training...\")\n","\n","    train_params = config['train_params']\n","\n","    optimizer_generator = torch.optim.Adam(generator.parameters(), lr=train_params['lr_generator'], betas=(0.5, 0.999))\n","    optimizer_discriminator = torch.optim.Adam(discriminator.parameters(), lr=train_params['lr_discriminator'], betas=(0.5, 0.999))\n","    optimizer_kp_detector = torch.optim.Adam(kp_detector.parameters(), lr=train_params['lr_kp_detector'], betas=(0.5, 0.999))\n","\n","    if checkpoint is not None:\n","        start_epoch = load_cpk(checkpoint, generator, discriminator, kp_detector, optimizer_generator, optimizer_discriminator, None if train_params['lr_kp_detector'] == 0 else optimizer_kp_detector)\n","    else:\n","        start_epoch = 0\n","\n","    scheduler_generator = MultiStepLR(optimizer_generator, train_params['epoch_milestones'], gamma=0.1, last_epoch=start_epoch - 1)\n","\n","    scheduler_discriminator = MultiStepLR(optimizer_discriminator, train_params['epoch_milestones'], gamma=0.1, last_epoch=start_epoch - 1)\n","\n","    scheduler_kp_detector = MultiStepLR(optimizer_kp_detector, train_params['epoch_milestones'], gamma=0.1, last_epoch=-1 + start_epoch * (train_params['lr_kp_detector'] != 0))\n","\n","    if 'num_repeats' in train_params or train_params['num_repeats'] != 1:\n","        dataset = DatasetRepeater(dataset, train_params['num_repeats'])\n","\n","    dataloader = DataLoader(dataset, batch_size=train_params['batch_size'], shuffle=True, num_workers=6, drop_last=True)\n","\n","    generator_full = GeneratorFullModel(kp_detector, generator, discriminator, train_params)\n","    discriminator_full = DiscriminatorFullModel(kp_detector, generator, discriminator, train_params)\n","\n","    if torch.cuda.is_available():\n","        generator_full = DataParallelWithCallback(generator_full, device_ids=device_ids)\n","        discriminator_full = DataParallelWithCallback(discriminator_full, device_ids=device_ids)\n","\n","    with Logger(log_dir=log_dir, visualizer_params=config['visualizer_params'], checkpoint_freq=train_params['checkpoint_freq']) as logger:\n","        for epoch in trange(start_epoch, train_params['num_epochs']):\n","            for x in dataloader:\n","                #print(dataloader)\n","                losses_generator, generated = generator_full(x)\n","\n","                loss_values = [val.mean() for val in losses_generator.values()]\n","                loss = sum(loss_values)\n","\n","                loss.backward()\n","                optimizer_generator.step()\n","                optimizer_generator.zero_grad()\n","                optimizer_kp_detector.step()\n","                optimizer_kp_detector.zero_grad()\n","\n","                if train_params['loss_weights']['generator_gan'] != 0:\n","                    optimizer_discriminator.zero_grad()\n","                    losses_discriminator = discriminator_full(x, generated)\n","                    loss_values = [val.mean() for val in losses_discriminator.values()]\n","                    loss = sum(loss_values)\n","\n","                    loss.backward()\n","                    optimizer_discriminator.step()\n","                    optimizer_discriminator.zero_grad()\n","                else:\n","                    losses_discriminator = {}\n","\n","                losses_generator.update(losses_discriminator)\n","                losses = {key: value.mean().detach().data.cpu().numpy() for key, value in losses_generator.items()}\n","                logger.log_iter(losses=losses)\n","\n","            scheduler_generator.step()\n","            scheduler_discriminator.step()\n","            scheduler_kp_detector.step()\n","            \n","            logger.log_epoch(epoch, {'generator': generator, 'discriminator': discriminator, 'kp_detector': kp_detector, 'optimizer_generator': optimizer_generator, 'optimizer_discriminator': optimizer_discriminator, 'optimizer_kp_detector': optimizer_kp_detector}, inp=x, out=generated)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fa9c3c41a1044acaeac4faa327f9042","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [4:40:53<00:00, 1685.37s/it]\n"]}]},{"cell_type":"markdown","metadata":{"id":"OUmH3gW9TB9v"},"source":["# CHECKPOINT READING"]},{"cell_type":"markdown","metadata":{"id":"5AtkUjO7bcVJ"},"source":["Showing the current training model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZRbPvAF4ip9","executionInfo":{"status":"ok","timestamp":1633064769140,"user_tz":300,"elapsed":2106,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"b7543f9d-9825-4e63-d738-53ad5d25e580"},"source":["checkpoint_path = './logs/00000009-checkpoint.pth.tar'\n","\n","config_path = './config/vox-adv-256.yaml'\n","generator1, kp_detector1 = load_checkpoints(config_path,checkpoint_path)\n","print(kp_detector1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataParallelWithCallback(\n","  (module): KPDetector(\n","    (predictor): Hourglass(\n","      (encoder): Encoder(\n","        (down_blocks): ModuleList(\n","          (0): DownBlock2d(\n","            (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (1): DownBlock2d(\n","            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (2): DownBlock2d(\n","            (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (3): DownBlock2d(\n","            (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","          (4): DownBlock2d(\n","            (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (pool): AvgPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0)\n","          )\n","        )\n","      )\n","      (decoder): Decoder(\n","        (up_blocks): ModuleList(\n","          (0): UpBlock2d(\n","            (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (1): UpBlock2d(\n","            (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (2): UpBlock2d(\n","            (conv): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (3): UpBlock2d(\n","            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","          (4): UpBlock2d(\n","            (conv): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","            (norm): SynchronizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          )\n","        )\n","      )\n","    )\n","    (kp): Conv2d(35, 10, kernel_size=(7, 7), stride=(1, 1))\n","    (jacobian): Conv2d(35, 40, kernel_size=(7, 7), stride=(1, 1))\n","    (down): AntiAliasInterpolation2d()\n","  )\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"6A2JrGCbTJfn"},"source":["# ANIMATION"]},{"cell_type":"markdown","metadata":{"id":"ABlALkATbgHR"},"source":["If we want a test of the model with the generated checkpoint, a source image and a driver video are saved"]},{"cell_type":"code","metadata":{"id":"6X4iH5BZTF3N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633064775786,"user_tz":300,"elapsed":5896,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"08efab91-f4c6-4a1e-dc81-361f55b50086"},"source":["from animate import normalize_kp\n","from skimage.transform import resize\n","from moviepy import editor\n","from tqdm import tqdm\n","import os"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n","Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n","Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1351680/45929032 bytes (2.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5349376/45929032 bytes (11.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b9428992/45929032 bytes (20.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b13328384/45929032 bytes (29.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b17252352/45929032 bytes (37.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b21372928/45929032 bytes (46.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25444352/45929032 bytes (55.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b29515776/45929032 bytes (64.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b33660928/45929032 bytes (73.3%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b37756928/45929032 bytes (82.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41893888/45929032 bytes (91.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n","  Done\n","File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lkR-WCpTTK41","executionInfo":{"status":"ok","timestamp":1633064785422,"user_tz":300,"elapsed":9649,"user":{"displayName":"Jose Ysique Neciosup","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02109369839046454245"}},"outputId":"509247c4-6dc1-4330-ff63-d9d31d418771"},"source":["adapt_movement_scale = True \n","cpu = False\n","relative = True\n","\n","D_VIDEO_PATH = 'logs/animation/video_d.mp4'\n","S_IMAGE_PATH = 'logs/animation/image_s.png'\n","\n","G_VIDEO_PATH = 'logs/animation/video_g.mp4'\n","if os.path.isfile(G_VIDEO_PATH):\n","  os.remove(G_VIDEO_PATH)\n","\n","source_image = imageio.imread(S_IMAGE_PATH)\n","driving_video = imageio.mimread(D_VIDEO_PATH, memtest=False)\n","\n","reader = imageio.get_reader(D_VIDEO_PATH)\n","driving_video_FPS = reader.get_meta_data()['fps']\n","\n","source_image = resize(source_image, (256, 256))[..., :3]\n","driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n","driving_video_audio = editor.AudioFileClip(D_VIDEO_PATH)\n","\n","\n","with torch.no_grad():\n","  predictions = []\n","  source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n","  if not cpu:\n","    source = source.cuda()\n","    driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n","    kp_source = kp_detector1(source)\n","    kp_driving_initial = kp_detector1(driving[:, :, 0])\n","\n","    for frame_idx in tqdm(range(driving.shape[2])):\n","      percentage = (frame_idx + 1)/ driving.shape[2]\n","      #yield \"data:\" + str(percentage) + \"\\n\\n\"\n","      driving_frame = driving[:, :, frame_idx]\n","      if not cpu:\n","        driving_frame = driving_frame.cuda()\n","      kp_driving = kp_detector1(driving_frame)\n","      kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n","              kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n","              use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n","      out = generator1(source, kp_source=kp_source, kp_driving=kp_norm)\n","\n","      predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 150/150 [00:05<00:00, 26.87it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"pDPOrAo0bkuo"},"source":["Saving de video generated"]},{"cell_type":"code","metadata":{"id":"6TWciTVJTNHH"},"source":["imageio.mimsave(G_VIDEO_PATH, [img_as_ubyte(frame) for frame in predictions], fps=driving_video_FPS)"],"execution_count":null,"outputs":[]}]}